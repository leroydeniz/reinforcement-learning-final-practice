{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71284c40",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right!important;\">M2.883 · Aprendizaje por refuerzo</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Máster universitario en Ciencia de datos (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Nombre y apellidos:</strong> Leroy Deniz Pedreira\n",
    "</div>\n",
    "\n",
    "# Práctica:\n",
    "\n",
    "## Implementación de un agente para la robótica espacial\n",
    "\n",
    "A lo largo de las tres partes de la asignatura hemos entrado en contacto con diferentes clases de algoritmos de aprendizaje por refuerzo que permiten solucionar problemas de control en una gran variedad de entornos.\n",
    "Esta práctica, que se va a extender a lo largo de un mes aproximadamente, da la posibilidad de enfrentarse al diseño de un agente para solucionar un caso específico de robótica.\n",
    "\n",
    "Atacaremos el problema a partir de la exploración del entorno y sus observaciones. Luego, pasaremos a la selección del algoritmo más oportuno para solucionar el entorno en cuestión. Finalmente, pasaremos por el entrenamiento y la prueba del agente, hasta llegar al análisis de su rendimiento.\n",
    "\n",
    "Para ello, se presentará antes el entorno de referencia, y luego, se pasará a la implementación de un agente Deep Q-Network (DQN) que lo solucione. Después de estas dos primeras fases de toma de contacto con el problema, se buscará otro agente que pueda mejorar el rendimiento del agente DQN anteriormente implementado.\n",
    "\n",
    "## Entorno \n",
    "\n",
    "Estamos trabajando sobre un problema de robótica espacial y en particular queremos solucionar el problema de aterrizaje propio, por ejemplo, de drones autónomos.\n",
    "\n",
    "Para ello, se elige lunar-lander como entorno simplificado. El entorno se puede encontrar en el siguiente enlace: https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
    "\n",
    "Lunar Lander consiste en una nave espacial que debe aterrizar en un lugar determinado del campo de observación. El agente conduce la nave y su objetivo es conseguir aterrizar en la pista de aterrizaje, coordenadas (0,0), y llegar con velocidad 0.\n",
    "\n",
    "La nave consta de tres motores (izquierda, derecha y el principal que tiene debajo) que le permiten ir corrigiendo su rumbo hasta llegar a destino.\n",
    "\n",
    "Las acciones que puede realizar la nave (espacio de acciones) son discretas.\n",
    "\n",
    "Las recompensas obtenidas a lo largo del proceso de aterrizaje dependen de las acciones que se toman y del resultado que se deriva de ellas.\n",
    "\n",
    "- Desplazarse de arriba a abajo, hasta la zona de aterrizaje, puede resultar en [+100,+140] puntos\n",
    "- Si se estrella al suelo, pierde -100 puntos\n",
    "- Si consigue aterrizar en la zona de aterrizaje (velocidad 0), gana +100 puntos\n",
    "- Si aterriza, pero no en la zona de aterrizaje (fuera de las banderas amarillas) se pierden puntos\n",
    "- El contacto de una pata con el suelo recibe +10 puntos (si se pierde contacto después de aterrizar, se pierden puntos)\n",
    "- Cada vez que enciende el motor principal pierde -0.3 puntos\n",
    "- Cada vez que enciende uno de los motores de izquierda o derecha, pierde -0.03 puntos\n",
    "\n",
    "La solución óptima es aquella en la que el agente, con un desplazamiento eficiente, consigue aterrizar en la zona de aterrizaje (0,0), tocando con las dos patas en el suelo y con velocidad nula. Se considera que el agente ha aprendido a realizar la tarea (i.e. el “juego” termina) cuando obtiene una media de al menos 200 puntos durante 100 episodios consecutivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4596fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install pygame\n",
    "!pip3 install \"gym[box2d]\"\n",
    "!pip3 install imageio\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scikit-image\n",
    "!pip3 install rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995cb3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.10.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "La versión de gym instala: 0.25.0\n",
      "El entorno utiliza:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerías.\n",
    "import warnings\n",
    "import pygame\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "pygame.display.set_mode((640,480))\n",
    "import pdb\n",
    "import gym\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "from IPython import display\n",
    "\n",
    "from skimage import transform\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "\n",
    "from math import inf\n",
    "from tqdm import tqdm # Librería para mostrar avance en iteraciones\n",
    "import datetime # Para medir el tiempo de ejecución\n",
    "import statistics as std # Para utilizar funciones como mean\n",
    "from collections import deque # Manejo de pilas con un máximo de n elementos, definidos\n",
    "\n",
    "# Comprobación de la versión de GYM instalada\n",
    "print('La versión de gym instala: ' + gym.__version__)\n",
    "# Comprobación de entorno con gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"El entorno utiliza: \", device)\n",
    "\n",
    "\n",
    "# Constantes\n",
    "MAX_EPISODES = 5000\n",
    "MAX_SCORE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f2b84",
   "metadata": {},
   "source": [
    "## Ejercicio 1.1\n",
    "\n",
    "Se pide explorar el entorno yt representar una ejecución aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe82eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00570612,  1.3990337 ,  0.5779653 , -0.5282997 , -0.0066053 ,\n",
       "       -0.13091765,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se crea el entorno\n",
    "warnings.filterwarnings('ignore')\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3264ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umbral de recompensas: (-inf, inf)\n",
      "Máximo número de pasos por episodio: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Umbral de recompensas: {env.reward_range}\")\n",
    "print(f\"Máximo número de pasos por episodio: {env.spec.max_episode_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf0dded",
   "metadata": {},
   "source": [
    "El espacio de acciones de este entorno consta de 4 opciones:\n",
    "\n",
    "\n",
    "- no hacer nada\n",
    "- activar el motor izquierdo\n",
    "- activar el motor principal debajo\n",
    "- activar el motor derecho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d3f77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión del espacio de acciones: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dimensión del espacio de acciones: {env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d3373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método para generar la imagen a partir de un estado con un texto informativo.\n",
    "def _label_with_text(frame):\n",
    "    '''\n",
    "    frame: estado de un entorno GYM.\n",
    "    '''\n",
    "    im = Image.fromarray(frame[0])\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "    return im\n",
    "    \n",
    "    \n",
    "#Método que permite crear un gif con la evolución de una partida dado un entorno GYM.\n",
    "def save_agent_gif(env, filename='random_lunar_lander', network = None):\n",
    "    frames = []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    ###########################################\n",
    "    #Jugar una partida aleatoria.\n",
    "    while not done:\n",
    "        action = env.action_space.sample() if network is None else network.get_action(state)\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        frames.append(_label_with_text(frame))\n",
    "        state, _, done, _ = env.step(action)\n",
    "    ##############################################\n",
    "\n",
    "    env.close()\n",
    "    imageio.mimwrite(os.path.join('./videos/', f'{filename}.gif'), frames, fps=60)\n",
    "    \n",
    "try:\n",
    "    os.makedirs('videos')\n",
    "except:\n",
    "    pass\n",
    "save_agent_gif(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b6208d",
   "metadata": {},
   "source": [
    "![title](videos/random_lunar_lander.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c6bbc",
   "metadata": {},
   "source": [
    "## Ejercicio 1.2\n",
    "\n",
    "Explicar los posibles espacios de observaciones y de acciones (informe escrito)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c79ce",
   "metadata": {},
   "source": [
    "Cada estado es un vector de ocho dimensiones, definidos en el siguiente orden:\n",
    "\n",
    "- posición _x_ de la nave\n",
    "- posición _y_ de la nave\n",
    "- velicidad lineal en _x_\n",
    "- velocidad lineal en _y_\n",
    "- ángulo de movimiento\n",
    "- velocidad angular de la nave\n",
    "- booleano que se activa si la pata izquierda toca el suelo lunar \n",
    "- booleano que se activa si la pata derecha toca el suelo lunar \n",
    "\n",
    "El vector _observation\\_low_ ofrece las cotas inferiores que pueden tomar cada uno de los parémetros, mostrando lo opuesto el vector _observation\\_high_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7bd6e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (8,)\n",
      "Observation high: [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ]\n",
      "Observation low: [-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observation shape: {env.observation_space.shape}\")\n",
    "print(f\"Observation high: {env.observation_space.high}\")\n",
    "print(f\"Observation low: {env.observation_space.low}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab852194",
   "metadata": {},
   "source": [
    "## Ejercicio 2.1\n",
    "\n",
    "Implementar un agente DQN para el entorno lunar-lander."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098edd8d",
   "metadata": {},
   "source": [
    "### Definición de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c737c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        n_inputs: tamaño del espacio de estados\n",
    "        n_outputs: tamaño del espacio de acciones\n",
    "        actions: array de acciones posibles\n",
    "        device: cpu o cuda\n",
    "        red_lineal: definición de la red lineal\n",
    "        \"\"\"\n",
    "        #######################################\n",
    "        ###TODO: inicialización y modelo###\n",
    "        self.input_shape = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "            \n",
    "        #######################################\n",
    "        ##TODO: Construcción de la red neuronal lineal completamente conectada\n",
    "        self.red_lineal = nn.Sequential(\n",
    "            nn.Linear(self.input_shape, 512, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.n_outputs, bias=True)\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.red_lineal.cuda()\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        ##TODO: Inicializar el optimizador\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    ### e-greedy method\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)\n",
    "            action = torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array(state)\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        return self.red_lineal(state_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ac43ff",
   "metadata": {},
   "source": [
    "### Definición del Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "555831a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "class experienceReplayBuffer:\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.buffer = namedtuple('Buffer',\n",
    "            field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size,\n",
    "                                   replace=False)\n",
    "        # Use el operador asterisco para desempaquetar deque\n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        return batch\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.replay_memory.append(\n",
    "            self.buffer(state, action, reward, done, next_state))\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78927deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy, copy\n",
    "\n",
    "class DQNAgent:\n",
    "    ###### Declaración de variables ##################\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        main_network,\n",
    "        buffer,\n",
    "        reward_threshold,\n",
    "        epsilon=0.1,\n",
    "        eps_decay=0.99,\n",
    "        batch_size=32,\n",
    "        nblock=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorno\n",
    "        main_network: clase con la red neuronal diseñada\n",
    "        target_network: red objetivo\n",
    "        buffer: clase con el buffer de repetición de experiencias\n",
    "        epsilon: epsilon\n",
    "        eps_decay: epsilon decay\n",
    "        batch_size: batch size\n",
    "        nblock: bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        reward_threshold: umbral de recompensa\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda\"\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "            \n",
    "        self.env = env\n",
    "        self.main_network = main_network\n",
    "        self.target_network = deepcopy(\n",
    "            main_network\n",
    "        )  # red objetivo (copia de la principal)\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.nblock = nblock  # últimos episodios a evaluar\n",
    "        self.reward_threshold = reward_threshold\n",
    "        self.initialize()\n",
    "\n",
    "    ##### Inicializar variables extra que se necesiten######\n",
    "    def initialize(self):\n",
    "        self.sync_eps = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.epsilon_history = []\n",
    "        self.update_loss = []\n",
    "        self.losses = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.training_rewards = []\n",
    "        self.state0 = self.env.reset()\n",
    "\n",
    "    ###### Tomar nueva acción ###############################################\n",
    "    def take_step(self, eps, mode=\"train\"):\n",
    "        if mode == \"explore\":\n",
    "            action = self.env.action_space.sample()  # acción aleatoria en el burn-in\n",
    "        else:\n",
    "            action = self.main_network.get_action(\n",
    "                self.state0, eps\n",
    "            )  # acción a partir del valor de Q (elección de la acción con mejor Q)\n",
    "            self.step_count += 1\n",
    "\n",
    "        # Realización de la acción y obtención del nuevo estado y la recompensa.\n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        self.buffer.append(\n",
    "            self.state0, action, reward, done, new_state\n",
    "        )  # guardar experiencia en el buffer\n",
    "        self.state0 = new_state\n",
    "\n",
    "        # Resetear entorno 'if done'\n",
    "        if done:\n",
    "            self.state0 = env.reset()\n",
    "        return done\n",
    "\n",
    "    ## Entrenamiento\n",
    "    def train(\n",
    "        self,\n",
    "        gamma=0.99,\n",
    "        max_episodes=50000,\n",
    "        batch_size=32,\n",
    "        dnn_update_frequency=4,\n",
    "        dnn_sync_frequency=2000,\n",
    "        min_episodios=250,\n",
    "        min_epsilon=0.01,\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        # Rellenamos el buffer con N experiencias aleatorias ()\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode=\"explore\")\n",
    "\n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        maximo = -inf\n",
    "        while training:\n",
    "            self.state0 = env.reset()\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # El agente toma una acción\n",
    "                gamedone = self.take_step(self.epsilon, mode=\"train\")\n",
    "                \n",
    "                #################################################################################\n",
    "                ### Actualizar red principal según la frecuencia establecida#####\n",
    "                if self.step_count % dnn_update_frequency == 0:\n",
    "                    self.update()\n",
    "\n",
    "                ########################################################################################\n",
    "                ### Sincronizar red principal y red objetivo según la frecuencia establecida#####\n",
    "                if self.step_count % dnn_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "                    self.sync_eps.append(episode)\n",
    "\n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    ##################################################################\n",
    "                    ######## Almacenar epsilon, training rewards y loss   #######\n",
    "                    self.epsilon_history.append(self.epsilon)\n",
    "                    self.training_rewards.append(self.total_reward)\n",
    "                    self.losses.append(np.mean(self.update_loss))\n",
    "                    self.update_loss = []\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ### Calcular la media de recompensa de los últimos X episodios, y almacenar#####\n",
    "                    mean_rewards = np.mean(  # calculamos la media de recompensa de los últimos X episodios\n",
    "                        self.training_rewards[-self.nblock :]\n",
    "                    )\n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "                    maximo = mean_rewards if mean_rewards > maximo else maximo\n",
    "\n",
    "                    ##################################################################\n",
    "\n",
    "                    print(f\"\\rEpisode {episode} Mean Rewards {mean_rewards:.2f} Epsilon {self.epsilon} , Maximo {maximo:.2f}\\t\\t\", end=\"\")\n",
    "\n",
    "                    # Comprobar si se ha llegado al máximo de episodios\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print(\"\\nEpisode limit reached.\")\n",
    "                        break\n",
    "\n",
    "                    # Termina el juego si la media de recompensas ha llegado al umbral fijado para este juego\n",
    "                    # y se ha entrenado un mínimo de episodios\n",
    "                    if (\n",
    "                        mean_rewards >= self.reward_threshold\n",
    "                        and min_episodios < episode\n",
    "                    ):\n",
    "                        training = False\n",
    "                        print(f\"\\nEnvironment solved in {episode} episodes!\")\n",
    "                        break\n",
    "\n",
    "                    #################################################################################\n",
    "                    ###### Actualizar epsilon ########\n",
    "                    self.epsilon = max(self.epsilon * self.eps_decay, 0.01)\n",
    "\n",
    "    ####################################\n",
    "    ##### Cálculo de la pérdida ####\n",
    "    def calculate_loss(self, batch):\n",
    "        # Separamos las variables de la experiencia y las convertimos a tensores\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(device=self.device)\n",
    "        actions_vals = (\n",
    "            torch.LongTensor(np.array(actions)).to(device=self.device).reshape(-1, 1)\n",
    "        )\n",
    "        dones_t = torch.ByteTensor(dones).to(device=self.device)\n",
    "\n",
    "        # Obtenemos los valores de Q de la red principal\n",
    "        qvals = torch.gather(self.main_network.get_qvals(states), 1, actions_vals)\n",
    "        # Obtenemos los valores de Q de la red objetivo El parametro detach() evita que estos valores actualicen la red objetivo\n",
    "        qvals_next = torch.max(self.target_network.get_qvals(next_states), dim=-1)[\n",
    "            0\n",
    "        ].detach()\n",
    "        qvals_next[dones_t] = 0  # 0 en estados terminales\n",
    "\n",
    "\n",
    "        #################################################################################\n",
    "        ###TODO: Calculamos ecuación de Bellman\n",
    "        expected_qvals = self.gamma * qvals_next + rewards_vals\n",
    "\n",
    "        # Cálculo de pérdida\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1, 1))\n",
    "        return loss\n",
    "\n",
    "    def update(self):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminamos cualquier gradiente pasado\n",
    "        batch = self.buffer.sample_batch(\n",
    "            batch_size=self.batch_size\n",
    "        )  # seleccionamos un conjunto del buffer\n",
    "        loss = self.calculate_loss(batch)  # calculamos la pérdida\n",
    "        loss.backward()  # hacemos la diferencia para obtener los gradientes\n",
    "        self.main_network.optimizer.step()  # aplicamos los gradientes a la red neuronal\n",
    "        # Guardamos los valores de pérdida\n",
    "        if self.device == \"cuda\":\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb30517e",
   "metadata": {},
   "source": [
    "## Ejercicio 2.2\n",
    "Entrenar el agente DQN y buscar los valores de los hiperparámetros que obtengan un alto rendimiento del agente. Para ello, es necesario listar los hiperparámetros bajo estudio y presentar las gráficas de las métricas que describen el aprendizaje.\n",
    "\n",
    "Se entrena inicialmente el agente con los valores por defectpo de la práctica 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb1f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling replay buffer...\n",
      "Training...\n",
      "Episode 2192 Mean Rewards -151.58 Epsilon 0.01 , Maximo -92.72\t\tMaximo -92.72\t\t\t"
     ]
    }
   ],
   "source": [
    "lr = 0.001            #Velocidad de aprendizaje\n",
    "BATCH_SIZE = 32       #Conjunto a coger del buffer para la red neuronal\n",
    "MEMORY_SIZE = 8000    #Máxima capacidad del buffer\n",
    "GAMMA = 0.99          #Valor gamma de la ecuación de Bellman\n",
    "EPSILON = 1           #Valor inicial de epsilon\n",
    "EPSILON_DECAY = .995  #Decaimiento de epsilon\n",
    "BURN_IN = 100         #Número de episodios iniciales usados para rellenar el buffer antes de entrenar\n",
    "MIN_EPISODES = 250    #Número mínimo de episodios\n",
    "DNN_UPD = 100         #Frecuencia de actualización de la red neuronal \n",
    "DNN_SYNC = 5000       #Frecuencia de sincronización de pesos entre la red neuronal y la red objetivo\n",
    "\n",
    "# Cargar el buffer de repetición de experiencias\n",
    "buffer = experienceReplayBuffer(memory_size=MEMORY_SIZE, burn_in=BURN_IN)\n",
    "\n",
    "# Cargar el modelo de red neuronal\n",
    "dqn_0 = DQNNetwork(env, learning_rate=lr)\n",
    "\n",
    "# Cargar el agente\n",
    "agent_0 = DQNAgent(env, dqn_0, buffer, MAX_SCORE, EPSILON, EPSILON_DECAY, BATCH_SIZE)\n",
    "\n",
    "# Entrenar el agente\n",
    "agent_0.train(gamma=GAMMA, max_episodes=MAX_EPISODES, batch_size=BATCH_SIZE, dnn_update_frequency=DNN_UPD, dnn_sync_frequency=DNN_SYNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb5a1d",
   "metadata": {},
   "source": [
    "### Exportación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.main_network.state_dict(), './models/dqn_agent_0.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb856f3",
   "metadata": {},
   "source": [
    "### Partida a través del modelo obtenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5377bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_agent_gif(env, 'dqn_lunar_lander_0', agent_0.target_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56aca59",
   "metadata": {},
   "source": [
    "![title](videos/dqn_lunar_lander_0.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923e529",
   "metadata": {},
   "source": [
    "### Recompensas obtenidas a lo largo del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(agent_0.training_rewards, label='Rewards')\n",
    "plt.plot(agent_0.mean_training_rewards, label='Mean Rewards')\n",
    "plt.axhline(agent_0.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "plt.axhline(0, color='#FF000099', label=\"Min positive reward\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfbb284",
   "metadata": {},
   "source": [
    "### Evolución de la pérdida a lo largo del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f0400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(agent_0.losses, label='Loss')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070550c",
   "metadata": {},
   "source": [
    "### Evolución de epsilon a lo largo del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9898df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(agent_0.epsilon_history, label='Epsilon')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b80a2",
   "metadata": {},
   "source": [
    "## Juego de pruebas para mejorar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36be74",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Configuración 0</td>\n",
    "        <td>Configuración 1</td>\n",
    "        <td>Configuración 2</td>\n",
    "        <td>Configuración 3</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>lr</td>\n",
    "        <td>0.001</td>\n",
    "        <td>0.001</td>\n",
    "        <td>0.1</td>\n",
    "        <td>0.01</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>DNN_UPD</td>\n",
    "        <td>100</td>\n",
    "        <td>100</td>\n",
    "        <td>100</td>\n",
    "        <td>100</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>DNN_SYNC</td>\n",
    "        <td>5000</td>\n",
    "        <td>5000</td>\n",
    "        <td>5000</td>\n",
    "        <td>5000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>BATCH_SIZE</td>\n",
    "        <td>32</td>\n",
    "        <td>64</td>\n",
    "        <td>32</td>\n",
    "        <td>10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>GAMMA</td>\n",
    "        <td>0.99</td>\n",
    "        <td>0.99</td>\n",
    "        <td>0.001</td>\n",
    "        <td>0.99</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MAX_EPISODES</td>\n",
    "        <td>1000</td>\n",
    "        <td>1000</td>\n",
    "        <td>1000</td>\n",
    "        <td>1000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>MEAN TRAINING REWARD</strong></td>\n",
    "        <td><strong>-177.70</strong></td>\n",
    "        <td><strong>0</strong></td>\n",
    "        <td><strong>-265.79</strong></td>\n",
    "        <td><strong>0</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>MAX REWARD</strong></td>\n",
    "        <td><strong>201.58</strong></td>\n",
    "        <td><strong>0</strong></td>\n",
    "        <td><strong>40.48</strong></td>\n",
    "        <td><strong>0</strong></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155159b",
   "metadata": {},
   "source": [
    "### Configuración 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e37db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001            #Velocidad de aprendizaje\n",
    "BATCH_SIZE = 64       #Conjunto a coger del buffer para la red neuronal\n",
    "MEMORY_SIZE = 8000    #Máxima capacidad del buffer\n",
    "GAMMA = 0.99          #Valor gamma de la ecuación de Bellman\n",
    "EPSILON = 1           #Valor inicial de epsilon\n",
    "EPSILON_DECAY = .995  #Decaimiento de epsilon\n",
    "BURN_IN = 100         #Número de episodios iniciales usados para rellenar el buffer antes de entrenar\n",
    "MIN_EPISODES = 250    #Número mínimo de episodios\n",
    "DNN_UPD = 100         #Frecuencia de actualización de la red neuronal \n",
    "DNN_SYNC = 5000       #Frecuencia de sincronización de pesos entre la red neuronal y la red objetivo\n",
    "\n",
    "# Cargar el buffer de repetición de experiencias\n",
    "buffer = experienceReplayBuffer(memory_size=MEMORY_SIZE, burn_in=BURN_IN)\n",
    "\n",
    "# Cargar el modelo de red neuronal\n",
    "dqn_1 = DQNNetwork(env, learning_rate=lr)\n",
    "\n",
    "# Cargar el agente\n",
    "agent_1 = DQNAgent(env, dqn_1, buffer, MAX_SCORE, EPSILON, EPSILON_DECAY, BATCH_SIZE)\n",
    "\n",
    "# Entrenar el agente\n",
    "agent_1.train(gamma=GAMMA, max_episodes=MAX_EPISODES, batch_size=BATCH_SIZE, dnn_update_frequency=DNN_UPD, dnn_sync_frequency=DNN_SYNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f106bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent_1.main_network.state_dict(), './models/dqn_agent_1.pth')\n",
    "save_agent_gif(env, 'dqn_lunar_lander_1', agent_1.target_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc738a",
   "metadata": {},
   "source": [
    "![title](videos/dqn_lunar_lander_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aae0c7",
   "metadata": {},
   "source": [
    "### Configuración 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1            #Velocidad de aprendizaje\n",
    "BATCH_SIZE = 32       #Conjunto a coger del buffer para la red neuronal\n",
    "MEMORY_SIZE = 8000    #Máxima capacidad del buffer\n",
    "GAMMA = 0.001          #Valor gamma de la ecuación de Bellman\n",
    "EPSILON = 1           #Valor inicial de epsilon\n",
    "EPSILON_DECAY = .995  #Decaimiento de epsilon\n",
    "BURN_IN = 100         #Número de episodios iniciales usados para rellenar el buffer antes de entrenar\n",
    "MIN_EPISODES = 250    #Número mínimo de episodios\n",
    "DNN_UPD = 100         #Frecuencia de actualización de la red neuronal \n",
    "DNN_SYNC = 5000       #Frecuencia de sincronización de pesos entre la red neuronal y la red objetivo\n",
    "\n",
    "# Cargar el buffer de repetición de experiencias\n",
    "buffer = experienceReplayBuffer(memory_size=MEMORY_SIZE, burn_in=BURN_IN)\n",
    "\n",
    "# Cargar el modelo de red neuronal\n",
    "dqn_2 = DQNNetwork(env, learning_rate=lr)\n",
    "\n",
    "# Cargar el agente\n",
    "agent_2 = DQNAgent(env, dqn_2, buffer, MAX_SCORE, EPSILON, EPSILON_DECAY, BATCH_SIZE)\n",
    "\n",
    "# Entrenar el agente\n",
    "agent_2.train(gamma=GAMMA, max_episodes=MAX_EPISODES, batch_size=BATCH_SIZE, dnn_update_frequency=DNN_UPD, dnn_sync_frequency=DNN_SYNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent_2.main_network.state_dict(), './models/dqn_agent_2.pth')\n",
    "save_agent_gif(env, 'dqn_lunar_lander_2', agent_2.target_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9ab25",
   "metadata": {},
   "source": [
    "![title](videos/dqn_lunar_lander_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e2e6d",
   "metadata": {},
   "source": [
    "### Configuración 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eecbcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01            #Velocidad de aprendizaje\n",
    "BATCH_SIZE = 10       #Conjunto a coger del buffer para la red neuronal\n",
    "MEMORY_SIZE = 8000    #Máxima capacidad del buffer\n",
    "GAMMA = 0.99          #Valor gamma de la ecuación de Bellman\n",
    "EPSILON = 1           #Valor inicial de epsilon\n",
    "EPSILON_DECAY = .995  #Decaimiento de epsilon\n",
    "BURN_IN = 100         #Número de episodios iniciales usados para rellenar el buffer antes de entrenar\n",
    "MIN_EPISODES = 250    #Número mínimo de episodios\n",
    "DNN_UPD = 100         #Frecuencia de actualización de la red neuronal \n",
    "DNN_SYNC = 5000       #Frecuencia de sincronización de pesos entre la red neuronal y la red objetivo\n",
    "\n",
    "# Cargar el buffer de repetición de experiencias\n",
    "buffer = experienceReplayBuffer(memory_size=MEMORY_SIZE, burn_in=BURN_IN)\n",
    "\n",
    "# Cargar el modelo de red neuronal\n",
    "dqn_3 = DQNNetwork(env, learning_rate=lr)\n",
    "\n",
    "# Cargar el agente\n",
    "agent_3 = DQNAgent(env, dqn_3, buffer, MAX_SCORE, EPSILON, EPSILON_DECAY, BATCH_SIZE)\n",
    "\n",
    "# Entrenar el agente\n",
    "agent_3.train(gamma=GAMMA, max_episodes=MAX_EPISODES, batch_size=BATCH_SIZE, dnn_update_frequency=DNN_UPD, dnn_sync_frequency=DNN_SYNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2829f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent_3.main_network.state_dict(), './models/dqn_agent_3.pth')\n",
    "save_agent_gif(env, 'dqn_lunar_lander_3', agent_3.target_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0079731",
   "metadata": {},
   "source": [
    "![title](videos/dqn_lunar_lander_3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89fc851",
   "metadata": {},
   "source": [
    "### Comparación entre los tres modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf18527",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(agent_0.mean_training_rewards, label='DQN Configuración 0')\n",
    "plt.plot(agent_1.mean_training_rewards, label='DQN Configuración 1')\n",
    "plt.plot(agent_2.mean_training_rewards, label='DQN Configuración 2')\n",
    "plt.plot(agent_3.mean_training_rewards, label='DQN Configuración 3')\n",
    "plt.axhline(200, color='r', label=\"Reward threshold\")\n",
    "plt.axhline(0, color='#FF000099', label=\"Min positive reward\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Mean Rewards')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5e919",
   "metadata": {},
   "source": [
    "## Ejercicio 2.3\n",
    "Probar el agente entrenado en el entorno de prueba. Visualizar su comportamiento (a través de gráficas de las métricas más oportunas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la función para probar agentes\n",
    "def agent_run(env, ag):\n",
    "    \n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    while True:\n",
    "        action = ag.main_network.get_action(state, epsilon=0.0)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        t += 1\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f2e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar los episodios pedidos\n",
    "episodes = 100\n",
    "total_rewards = []\n",
    "\n",
    "for _ in tqdm(range(episodes), ncols=100, desc=\"Jugando\"):\n",
    "    reward = agent_run(env, agent_0)\n",
    "    total_rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc385f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(total_rewards, label='Rewards')\n",
    "plt.axhline(agent.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "plt.axhline(0, color='#FF000099', label=\"Min positive reward\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.xticks(range(0,episodes+1,5))\n",
    "plt.ylabel('Rewards')\n",
    "#plt.ylim([0,agent.reward_threshold*1.1])\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab71c19",
   "metadata": {},
   "source": [
    "## Propuesta de mejora\n",
    "\n",
    "En esta parte se pide implementar otro agente, entre aquellos que hemos visto a lo largo de la asignatura, que pueda solucionar el problema de robótica espacial de forma más eficiente con respecto al agente DQN.\n",
    "En particular, se pide solucionar los 3 puntos siguientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b06aa1",
   "metadata": {},
   "source": [
    "### Ejercicio 3.1\n",
    "\n",
    "Implementar el agente identificado en el entorno lunar-lander.\n",
    "Justificar las razones que han llevado a probar este tipo de agente. Detallar qué tipos de problemas se espera se puedan solucionar con respecto a la implementación DQN anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733b5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reinforce(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, env, learning_rate=1e-3, device='cpu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        n_inputs: tamaño del espacio de estados\n",
    "        n_outputs: tamaño del espacio de acciones\n",
    "        actions: array de acciones posibles\n",
    "        \"\"\"\n",
    "        \n",
    "        # Inicializar variables\n",
    "        self.input_shape = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        self.device = 'cpu'\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Definición de la red lineal\n",
    "        self.red_lineal = nn.Sequential(\n",
    "            nn.Linear(self.input_shape, 512, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, self.n_outputs, bias=True),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        # Habilitar cuda si está disponible\n",
    "        if self.device == 'cuda':\n",
    "            self.red_lineal.cuda()\n",
    "\n",
    "        # Optimizador Adam\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    #Obtención de las probabilidades de las posibles acciones\n",
    "    def get_action_prob(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array(state)\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        return self.red_lineal(state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e51302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "\n",
    "    # Declarar variables\n",
    "    def __init__(self, env, dnnetwork, nblock, reward_threshold):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorno\n",
    "        dnnetwork: clase con la red neuronal diseñada\n",
    "        nblock: bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        reward_threshold: umbral de recompensa definido en el entorno\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.main_network = dnnetwork\n",
    "        self.nblock = nblock\n",
    "        self.reward_threshold = reward_threshold\n",
    "        self.initialize()\n",
    "\n",
    "    # Inicializar variables extra que se necesiten\n",
    "    def initialize(self):\n",
    "        self.batch_rewards = []\n",
    "        self.batch_actions = []\n",
    "        self.batch_states = []\n",
    "        self.batch_counter = 1\n",
    "        \n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.update_loss = []\n",
    "        self.losses = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.state0 = self.env.reset()\n",
    "\n",
    "    ## Entrenamiento\n",
    "    def train(self, gamma=0.99, max_episodes=2000, batch_size=10):\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        episode, maximo = 0, -inf\n",
    "        action_space = np.arange(self.env.action_space.n)\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training:\n",
    "            state0 = self.env.reset()\n",
    "            episode_states = []\n",
    "            episode_rewards = []\n",
    "            episode_actions = []\n",
    "            gamedone = False\n",
    "\n",
    "            while gamedone == False:\n",
    "                # Tomar nueva acción\n",
    "                action_probs = self.main_network.get_action_prob(state0) #distribución de probabilidad de las acciones dado el estado actual\n",
    "                action_probs = action_probs.detach().numpy() #se quita la función de optimize, numpy convierte a numpy array, [0] toma únicamente el primer elemento de la matriz\n",
    "                action = np.random.choice(action_space, p=action_probs) #acción aleatoria de la distribución de probabilidad\n",
    "                next_state, reward, gamedone, _ = env.step(action)\n",
    "\n",
    "                # Almacenamos experiencias que se van obteniendo en este episodio\n",
    "                episode_states.append(state0)\n",
    "                episode_rewards.append(reward)\n",
    "                episode_actions.append(action)\n",
    "                state0 = next_state\n",
    "\n",
    "\n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    # Calculamos el término del retorno menos la línea de base\n",
    "                    self.batch_rewards.extend(self.discount_rewards(episode_rewards))\n",
    "                    self.batch_states.extend(episode_states)\n",
    "                    self.batch_actions.extend(episode_actions)\n",
    "                    self.batch_counter += 1\n",
    "                    self.training_rewards.append(sum(episode_rewards)) # guardamos las recompensas obtenidas\n",
    "\n",
    "                    # Calcular media de recompensas de los últimos X episodios, y almacenar\n",
    "                    mean_rewards = np.mean(self.training_rewards[-self.nblock:]) # calculamos la media de recompensa de los últimos X episodios\n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "                    maximo = mean_rewards if mean_rewards > maximo else maximo\n",
    "\n",
    "\n",
    "                    # Actualizamos la red cuando se completa el tamaño del batch\n",
    "                    if self.batch_counter == self.batch_size:\n",
    "                        self.update(self.batch_states, self.batch_rewards, self.batch_actions)\n",
    "\n",
    "                        # Almacenar training_loss\n",
    "                        self.losses.append(self.update_loss)\n",
    "\n",
    "                        self.update_loss = []\n",
    "\n",
    "                        # Reseteamos las variables del epsiodio\n",
    "                        self.batch_rewards = []\n",
    "                        self.batch_actions = []\n",
    "                        self.batch_states = []\n",
    "                        self.batch_counter = 1\n",
    "\n",
    "\n",
    "                    print(f\"\\rEpisode: {episode:d} Mean Rewards: {mean_rewards:.2f}\\t\\tMaximo: {maximo}\\t\\t\", end=\"\")\n",
    "\n",
    "                    # Comprobamos que todavía quedan episodios\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "\n",
    "                    # Termina el juego si la media de recompensas ha llegado al umbral fijado para este juego\n",
    "                    if mean_rewards >= self.reward_threshold:\n",
    "                        training = False\n",
    "                        print(f'\\nEnvironment solved in {episode} episodes!')\n",
    "                        break\n",
    "\n",
    "    # Cálculo del retorno menos la línea de base\n",
    "    def discount_rewards(self, rewards):\n",
    "        discount_r = np.zeros_like(rewards)\n",
    "        timesteps = range(len(rewards))\n",
    "        reward_sum = 0\n",
    "        for i in reversed(timesteps):  #revertimos la dirección del vector para hacer la suma cumulativa\n",
    "            reward_sum = rewards[i] + self.gamma*reward_sum\n",
    "            discount_r[i] = reward_sum\n",
    "        baseline = np.mean(discount_r) # establecemos la media de la recompensa como línea de base\n",
    "        return discount_r - baseline     \n",
    "\n",
    "\n",
    "    ## Actualización\n",
    "    def update(self, batch_s, batch_r, batch_a):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminamos cualquier gradiente pasado\n",
    "        state_t = torch.FloatTensor(batch_s)\n",
    "        reward_t = torch.FloatTensor(batch_r)\n",
    "        action_t = torch.LongTensor(batch_a)\n",
    "        loss = self.calculate_loss(state_t, action_t, reward_t) # calculamos la pérdida\n",
    "        loss.backward() # hacemos la diferencia para obtener los gradientes\n",
    "        self.main_network.optimizer.step() # aplicamos los gradientes a la red neuronal\n",
    "        # Guardamos los valores de pérdida\n",
    "        if self.main_network.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "\n",
    "    # Cálculo de la pérdida\n",
    "    # Recordatorio: cada actualización es proporcional al producto del retorno y el gradiente de la probabilidad\n",
    "    # de tomar la acción tomada, dividido por la probabilidad de tomar esa acción (logaritmo natural)\n",
    "    def calculate_loss(self, state_t, action_t, reward_t):\n",
    "        logprob = torch.log(self.main_network.get_action_prob(state_t))\n",
    "        selected_logprobs = reward_t * logprob[np.arange(len(action_t)), action_t]\n",
    "        loss = -selected_logprobs.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d4827",
   "metadata": {},
   "source": [
    "### Ejercicio 3.2\n",
    "\n",
    "Entrenar el agente identificado y buscar los valores de los hiperpárametros que obtengan el rendimiento “óptimo” del agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005            #Velocidad de aprendizaje\n",
    "BATCH_SIZE = 8        #Conjunto a coger del buffer para la red neuronal\n",
    "MEMORY_SIZE = 8000    #Máxima capacidad del buffer\n",
    "GAMMA = 0.99          #Valor gamma de la ecuación de Bellman\n",
    "MIN_EPISODES = 250    #Número mínimo de episodios\n",
    "\n",
    "# Cargar el modelo de red neuronal\n",
    "reinforce_dqn = Reinforce(env, learning_rate=lr)\n",
    "\n",
    "# Cargar el agente\n",
    "reinforce_agent = REINFORCEAgent(env, reinforce_dqn, 100, MAX_SCORE)\n",
    "\n",
    "# Entrenar el agente\n",
    "reinforce_agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(reinforce_agent.main_network.state_dict(), './models/reinforce_agent.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reinforce_agent_gif(env, ag, nombre_fichero):\n",
    "    '''\n",
    "    :param env:  entorno GYM\n",
    "    :param ag:  agente entrenado\n",
    "    :param nombre_fichero:  nombre del fichero\n",
    "    :return:\n",
    "    '''\n",
    "    frames = []\n",
    "    observation = env.reset()\n",
    "    action_space = np.arange(env.action_space.n)\n",
    "    state= observation\n",
    "    total_reward = 0\n",
    "    t=0\n",
    "    done = False\n",
    "    while not done:\n",
    "        state = observation\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        frames.append(_label_with_text(frame))\n",
    "        action_probs = ag.main_network.get_action_prob(state)\n",
    "        action_probs = action_probs.detach().numpy()\n",
    "        action = np.random.choice(action_space, p=action_probs)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        t=t+1\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    try:\n",
    "        os.makedirs('videos')\n",
    "    except:\n",
    "        pass\n",
    "    imageio.mimwrite(os.path.join('./videos/', nombre_fichero), frames, fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350becfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "save_reinforce_agent_gif(env, reinforce_agent, 'reinforce_agent_lunar_lander.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095f3da",
   "metadata": {},
   "source": [
    "![title](videos/reinforce_agent_lunar_lander.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187c937",
   "metadata": {},
   "source": [
    "### Test del agente REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_reinforce_run(env, ag):\n",
    "    observation = env.reset()\n",
    "    action_space = np.arange(env.action_space.n)\n",
    "    state= observation\n",
    "    total_reward = 0\n",
    "    t=0\n",
    "    done = False\n",
    "    while not done:\n",
    "        state = observation\n",
    "        action_probs = ag.main_network.get_action_prob(state) #distribución de probabilidad de las acciones dado el estado actual\n",
    "        action_probs = action_probs.detach().numpy() #se quita la función de optimize, numpy convierte a numpy array, [0] toma únicamente el primer elemento de la matriz\n",
    "        action = np.random.choice(action_space, p=action_probs) #acción aleatoria de la distribución de probabilidad\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        t=t+1\n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "# Ejecutar los episodios pedidos\n",
    "episodes = 20\n",
    "total_rewards = []\n",
    "\n",
    "for _ in tqdm(range(episodes), ncols=100, desc=\"Jugando\"):\n",
    "    reward = agent_reinforce_run(env, reinforce_agent)\n",
    "    total_rewards.append(reward)\n",
    "    \n",
    "# Calcular la suma de recompensas\n",
    "for i, suma in enumerate(total_rewards):\n",
    "    print(f\"Episode {i+1} - {total_rewards[i]}\")\n",
    "    \n",
    "# Gráficas de recompensas\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(total_rewards, label='Rewards')\n",
    "plt.axhline(reinforce_agent.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.xticks(range(episodes+1))\n",
    "plt.ylabel('Rewards')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylim([0,reinforce_agent.reward_threshold*1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f2cd3b",
   "metadata": {},
   "source": [
    "### Ejercicio 3.3\n",
    "\n",
    "Analizar el comportamiento del agente identificado entrenado en el entorno de prueba y compararlo con el agente implementado en el punto 2 (a través de gráficas de las métricas más oportunas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a21bb",
   "metadata": {},
   "source": [
    "### Recompensas obtenidas a lo largo del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ebe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(reinforce_agent.training_rewards, label='Rewards')\n",
    "plt.plot(reinforce_agent.mean_training_rewards, label='Mean Rewards')\n",
    "plt.axhline(reinforce_agent.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "plt.axhline(0, color='#FF000099', label=\"Min positive reward\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards'),0\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21bfee5",
   "metadata": {},
   "source": [
    "### Evolución de la pérdida a lo largo del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(reinforce_agent.losses, label='Loss')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e3b388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(agent_0.mean_training_rewards, label='DQN Configuración 0')\n",
    "plt.plot(agent_1.mean_training_rewards, label='DQN Configuración 1')\n",
    "plt.plot(agent_2.mean_training_rewards, label='DQN Configuración 2')\n",
    "plt.plot(agent_3.mean_training_rewards, label='DQN Configuración 3')\n",
    "plt.plot(reinforce_agent.mean_training_rewards, label='Mean Rewards')\n",
    "plt.axhline(200, color='r', label=\"Reward threshold\")\n",
    "plt.axhline(0, color='#FF000099', label=\"Min positive reward\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Mean Rewards')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a2d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5b756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f366c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412dc256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27852fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c664a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e8b6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2665f12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5201f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac4a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
